<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.6">Jekyll</generator><link href="http://localhost:4000/blog/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/blog/" rel="alternate" type="text/html" /><updated>2020-08-16T00:03:05+05:30</updated><id>http://localhost:4000/blog/</id><title type="html">Sukeesh</title><subtitle>Personal blog</subtitle><author><name>Sukeesh</name><email>vsukeeshbabu@gmail.com</email></author><entry><title type="html">Looking back; two years after graduation</title><link href="http://localhost:4000/blog/2020-08-14/software" rel="alternate" type="text/html" title="Looking back; two years after graduation" /><published>2020-08-14T00:00:00+05:30</published><updated>2020-08-14T00:00:00+05:30</updated><id>http://localhost:4000/blog/2020-08-14/software</id><content type="html" xml:base="http://localhost:4000/blog/2020-08-14/software">&lt;p&gt;It’s been more than 2 years since I graduated and started working as a Software Engineer at Mercari. So, I thought of putting down everything I did in the past 2 years after I moved to Tokyo, Japan in September 2018.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
My Journey so far has been good at Mercari.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/images/aug2020/mori.png&quot; alt=&quot;Image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The first month was mostly orientation and at the end of October, I joined the &lt;b&gt;US@Tokyo&lt;/b&gt; team as a Backend Engineer.&lt;/p&gt;

&lt;h2 id=&quot;initial-days&quot;&gt;Initial days&lt;/h2&gt;

&lt;p&gt;In the first quarter, I worked on Home API (Golang) in the Backend team. Integrating ML services to the backend. Setting up Prometheus and Grafana for ML Services written in Python Flask.&lt;/p&gt;

&lt;div class=&quot;language-diff highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gi&quot;&gt;+ Golang
+ Prometheus
+ Grafana
+ Kubernetes
+ Docker
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Below are the &lt;b&gt;few&lt;/b&gt; of the components I added on the home screen of mercari in my first quarter&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/images/aug2020/octdec18.png&quot; alt=&quot;Image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This was very rewarding since everything I write was visible on the home screen of the application.
&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;personal-hackathon-in-tokyo&quot;&gt;(Personal) Hackathon in Tokyo&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;https://upload.wikimedia.org/wikipedia/commons/b/b9/Tokyo_Big_Sight_at_Night.jpg&quot; alt=&quot;Tokyo Big Sight&quot; /&gt;
I participated in a hackathon for the first time in Tokyo and the atmosphere was completely different compared to that of India. Here is the link to my &lt;a href=&quot;https://devpost.com/software/smart-offline-ads&quot;&gt;submission for the Hackathon&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;code-code-code&quot;&gt;Code.. Code.. Code&lt;/h2&gt;
&lt;p&gt;Later year (2019) in the beginning, I spent time mostly on the Home screen API of the mercari. I worked on adding many many components on Home screen of the application. Here is one of &lt;b&gt;many&lt;/b&gt; components I developed. This particular component below gave me experience of working with Search backend.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/images/aug2020/janjun18.png&quot; alt=&quot;Image&quot; height=&quot;50%&quot; width=&quot;50%&quot; /&gt;&lt;/p&gt;

&lt;div class=&quot;language-diff highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gi&quot;&gt;+ Redis
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;new-microservice-tada&quot;&gt;New microservice :tada:&lt;/h2&gt;

&lt;p&gt;and then, I finally started working on co-creating a new microservice for saved search. During this first microservice of mine, My learning rate was exponential.&lt;/p&gt;

&lt;div class=&quot;language-diff highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gi&quot;&gt;+ Microservices
+ Cloud Spanner
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;personal-kubernetes&quot;&gt;(Personal) Kubernetes&lt;/h2&gt;

&lt;p&gt;And also this is when I started to deep dive into Kubernetes. I was super enthusiastic about Cloud native applications and I spent most of my free time reading code of the core &lt;a href=&quot;https://github.com/kubernetes/kubernetes/&quot;&gt;Kubernetes&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/images/aug2020/kubernetes.png&quot; alt=&quot;Image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I also started contributing to the Kubernetes open source on GitHub. Here, you can read on &lt;a href=&quot;https://sukeesh.com/blog/2019-08-01/started-kubernetes&quot;&gt;how I started contributing to Kubernetes&lt;/a&gt;.&lt;/p&gt;

&lt;div class=&quot;language-diff highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gi&quot;&gt;+ Istio
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;second-microservice&quot;&gt;Second microservice&lt;/h2&gt;

&lt;p&gt;In mid-2019, I started working on Price suggestion project which suggests a price range when an user is listing an item on the app.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/images/aug2020/ps.png&quot; alt=&quot;Image&quot; height=&quot;40%&quot; width=&quot;40%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I designed and developed a new backend microservice (written in Golang) for the Price suggestion Machine Learning model.&lt;/p&gt;

&lt;!-- ![SOE](https://upload.wikimedia.org/wikipedia/commons/b/b9/Sieve_of_Eratosthenes_animation.gif) --&gt;

&lt;p&gt;We released many many various features around this Price suggestion project and I was the backend engineer initially for all of the projects in this team.&lt;/p&gt;

&lt;h2 id=&quot;payments&quot;&gt;Payments&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;../assets/images/aug2020/enoshima.png&quot; alt=&quot;Image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;During this phase, I also worked parallelly with the Payments team briefly for a feature required in &lt;a href=&quot;https://www.mercari.com/us/digital/&quot;&gt;Mercari GG&lt;/a&gt;.&lt;/p&gt;

&lt;div class=&quot;language-diff highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gi&quot;&gt;+ PHP
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;personal-project-kubernetes-job-notifier&quot;&gt;(Personal) Project, Kubernetes Job Notifier&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;../assets/images/aug2020/k8sjob.png&quot; alt=&quot;Image&quot; height=&quot;60%&quot; width=&quot;60%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Monitoring Kubernetes cronjob was little tricky and instantaneous updates are hard. So, I thought of solving this problem. I made use of Kubernetes API and Slack API to create a slack notifier for Kubernetes cronjob failures/success. This project as usual :wink:, gathered few stars and I still keep getting “Thank you” emails from those who are using this tool of mine. &lt;a href=&quot;https://github.com/sukeesh/k8s-job-notify&quot;&gt;Here is the link to repository on GitHub&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;2020&quot;&gt;2020&lt;/h2&gt;

&lt;p&gt;Later in January and February of 2020, I worked on various Backend projects including &lt;a href=&quot;https://www.mercari.com/us/become-a-pro-seller/&quot;&gt;Mercari Pro seller&lt;/a&gt;.&lt;/p&gt;

&lt;div class=&quot;language-diff highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gi&quot;&gt;+ BigQuery
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h1 id=&quot;backend---machine-learning&quot;&gt;Backend -&amp;gt; Machine Learning&lt;/h1&gt;

&lt;p&gt;In &lt;b&gt;March 2020&lt;/b&gt;, I took a decision to move to Machine learning team from Backend team. A complete new vertical and I had literally zero professional experience in Machine learning at this point.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;Even after joining ML team, I was still working on Backend only tasks.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;So, this is when I decided to gain more theoretical and practical knowledge on ML. I took up the specialization on &lt;a href=&quot;deeplearning.ai&quot;&gt;deeplearning.ai&lt;/a&gt;. I worked hard for about a month to achieve this.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/images/aug2020/dlcert.png&quot; alt=&quot;Image&quot; height=&quot;70%&quot; width=&quot;70%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This gave me so many insights. It was very practical and I completely recommend this to anyone who is interested in starting their career with ML. &lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;my-first-machine-learning-model-rocket&quot;&gt;My First Machine learning model :rocket:&lt;/h2&gt;

&lt;p&gt;Soon after this, I started working on developing a new Machine learning model for price suggestion. I took up the challenge and was able to train, develop and deploy onto Production in about 1.5 months.&lt;/p&gt;

&lt;!-- ![Image](../assets/images/may2020/go_cpp.png) --&gt;
&lt;div class=&quot;language-diff highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gi&quot;&gt;+ Polyaxon
+ Many many ML frameworks
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;personal-finalist-in-a-hackathon&quot;&gt;(Personal) Finalist in a Hackathon&lt;/h2&gt;

&lt;p&gt;Again, time for some personal. &lt;br /&gt;&lt;br /&gt;
This time I participated in a Hackathon instead of working on a personal project. Generally, whenever I participate in a Hackathon, winning projects are mostly related to Machine Learning. &lt;br /&gt;&lt;br /&gt;
So, at this point of time since I knew ML and had practical knowledge. I thought of applying ML for the Hackathon project and Surprisingly, I was a &lt;b&gt;finalist&lt;/b&gt; and was &lt;b&gt;offered&lt;/b&gt; an opportunity for pre-incubation by an Indian Venture Capital firm.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/images/aug2020/hackathon.png&quot; alt=&quot;Image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The idea was simple. I developed a WhatsApp bot which fact checks messages related to the Coronavirus.&lt;/p&gt;

&lt;h2 id=&quot;image-recognition&quot;&gt;Image Recognition&lt;/h2&gt;

&lt;p&gt;Working with Images has taught me a lot of patience. This is already a running model. My role is more like maintaining the project. &lt;br /&gt;&lt;/p&gt;

&lt;p&gt;This project is the most interesting because I knew nothing before starting!!&lt;/p&gt;

&lt;div class=&quot;language-diff highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gi&quot;&gt;+ Kubeflow
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;hr /&gt;

&lt;p&gt;These are &lt;strong&gt;only few&lt;/strong&gt; of many projects which I worked on in the past two years. I learnt so many things as a Software engineer. I always tried to take over tasks which are hard/completely unknown to me because later when I look back, I always learnt many things by doing so.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Anything which aint hard, isnt worth doing it.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;some-useful-tips&quot;&gt;Some useful tips&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://qr.ae/TUtVDd&quot;&gt;What can I learn/know right now in 10 minutes that will be useful for the rest of my life?&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;</content><author><name>Sukeesh</name></author><summary type="html">It’s been more than 2 years since I graduated and started working as a Software Engineer at Mercari. So, I thought of putting down everything I did in the past 2 years after I moved to Tokyo, Japan in September 2018.</summary></entry><entry><title type="html">Go vs C++ | Which is faster? | Sieve of Eratosthenes</title><link href="http://localhost:4000/blog/2020-05-24/cpp-vs-go" rel="alternate" type="text/html" title="Go vs C++ | Which is faster? | Sieve of Eratosthenes" /><published>2020-05-24T00:00:00+05:30</published><updated>2020-05-24T00:00:00+05:30</updated><id>http://localhost:4000/blog/2020-05-24/cpp-vs-go</id><content type="html" xml:base="http://localhost:4000/blog/2020-05-24/cpp-vs-go">[![Image](../assets/images/may2020/gocbanner.png)](https://amzn.to/2ZJNefN)

Comparing a Sieve of Eratosthenes in Golang vs C++! (Two of my most favourite programming languages)

## Sieve of Eratosthenes

![SOE](https://upload.wikimedia.org/wikipedia/commons/b/b9/Sieve_of_Eratosthenes_animation.gif)

The intention of this post is not to explain the algorithm. You can read more about [it here](http://en.wikipedia.org/wiki/Sieve_of_Eratosthenes).

## Code

Code Implementation in both the languages

### &lt;a href=&quot;https://amzn.to/3cZ9sOD&quot;&gt;Golang&lt;/a&gt;

&lt;script src=&quot;https://gist.github.com/sukeesh/3fec59b9025978dbf92b6b6e66b92e6e.js&quot;&gt;&lt;/script&gt;

### &lt;a href=&quot;https://amzn.to/2ARSPGc&quot;&gt;C++&lt;/a&gt;

&lt;script src=&quot;https://gist.github.com/sukeesh/b9edbcba43f707fbefd9cb49e55d6926.js&quot;&gt;&lt;/script&gt;


## Hardware

- MacBook Pro (15-inch, 2018)
- Processor 2.9 GHz 6-Core Intel Core i9
- Memory 32 GB 2400 MHz DDR4


## Results!

I tested this code using the time function in a zsh shell.&lt;br&gt;

&lt;b&gt;Example usage of time function&lt;/b&gt;

```bash
$ time ./a.out 4194304
./a.out 4194304  0.05s user 0.00s system 95% cpu 0.057 total
```

I tested by generating [2, 4, 8, 16, 32, .... , 4194304] Prime numbers in both the programming languages. Here is how they performed. 

Until 131072, C++ was performing slightly better than Go. But, later Go started dominating and the curve went exponential.

![Image](../assets/images/may2020/go_cpp.png)</content><author><name>Sukeesh</name></author><summary type="html"></summary></entry><entry><title type="html">What would have happened if you would have brought these stocks daily?</title><link href="http://localhost:4000/blog/2020-05-22/stock-a-day" rel="alternate" type="text/html" title="What would have happened if you would have brought these stocks daily?" /><published>2020-05-22T00:00:00+05:30</published><updated>2020-05-22T00:00:00+05:30</updated><id>http://localhost:4000/blog/2020-05-22/stock-a-day</id><content type="html" xml:base="http://localhost:4000/blog/2020-05-22/stock-a-day">![BSE](../assets/images/may2020/bse.jpg &quot;Bombay Stock Exchange&quot;)

&lt;br&gt;&lt;br&gt;

## Disclaimer

&quot;I am NOT a SEBI registered advisor or a financial adviser. Any of my investment or trades I share on my blog are provided for educational purposes only and do not constitute specific financial, trading or investment advice&quot;

You should discuss your specific requirements and situation with a qualified financial adviser

&lt;br&gt;

----------

&lt;br&gt;

I tried to see how much Profit/Loss one would have made if he/she brought a unit of stock &lt;b&gt;daily&lt;/b&gt; in the following Instruments.

I picked the following randomly from the [NIFTY50 Index](https://www1.nseindia.com/live_market/dynaContent/live_watch/equities_stock_watch.htm).

&lt;br&gt;

## Motivation

&lt;i&gt;Burton Malkiel&lt;/i&gt; famously wrote in [A Random Walk Down Wall Street](https://amzn.to/2zXGVdu) that
&gt; A blindfolded monkey throwing darts at a newspaper's financial pages could select a portfolio that would do just as well as one carefully selected by the experts.

&lt;br&gt;

----------

## Assumptions

For each day, there is a low price and a high price. I have chosen the average value of these two as the buy price on that day. 

### Why average?

It’s impossible for anyone to predict the bottom of the chart in the stock market. So, anyone who has chosen the lowest price traded on that day is very lucky and anyone who has chosen the highest price traded on that day is very unlucky.


In the following charts, “Cum cost” represents the cumulative cost of all the units you brought so far and “Cum value” represents the current value of all the units you brought so far.

Let’s explore!

## [Tata Consultancy Services Limited](https://www1.nseindia.com/live_market/dynaContent/live_watch/get_quote/GetQuote.jsp?symbol=TCS)

![TCS Cum](../assets/images/may2020/tcs_cum.png &quot;TCS&quot;)
![TCS PL](../assets/images/may2020/tcs_pl.png &quot;TCS&quot;)

You would have made a profit of &lt;span style=&quot;color:green&quot;&gt;INR 5700&lt;/span&gt;!!.

-------------

## [Reliance Industries Limited](https://www1.nseindia.com/live_market/dynaContent/live_watch/get_quote/GetQuote.jsp?symbol=RELIANCE)

![RIL Cum](../assets/images/may2020/ril_cum.png &quot;RIL&quot;)
![RIL PL](../assets/images/may2020/ril_pl.png &quot;RIL&quot;)

You would have made a profit of around &lt;span style=&quot;color:green&quot;&gt;INR 10000&lt;/span&gt;!!

-------------

## [HDFC Bank Limited](https://www1.nseindia.com/live_market/dynaContent/live_watch/get_quote/GetQuote.jsp?symbol=HDFCBANK)

![HDFC Cum](../assets/images/may2020/hdfc_cum.png &quot;hdfc&quot;)
![HDFC PL](../assets/images/may2020/hdfc_pl.png &quot;hdfc&quot;)

You would have made a LOSS of around &lt;span style=&quot;color:red&quot;&gt;INR 5300&lt;/span&gt;!!

-------------

## [Dr. Reddy's Laboratories Limited](https://www1.nseindia.com/live_market/dynaContent/live_watch/get_quote/GetQuote.jsp?symbol=DRREDDY)

![DR Cum](../assets/images/may2020/dr_cum.png &quot;DR&quot;)
![DR PL](../assets/images/may2020/ril_pl.png &quot;DR&quot;)

You would have made a profit of around &lt;span style=&quot;color:green&quot;&gt;INR 26000&lt;/span&gt;!!

-------------

## [State Bank of India](https://www1.nseindia.com/live_market/dynaContent/live_watch/get_quote/GetQuote.jsp?symbol=SBIN)

![SBIN Cum](../assets/images/may2020/sbi_cum.png &quot;SBIN&quot;)
![SBIN PL](../assets/images/may2020/sbi_pl.png &quot;SBIN&quot;)

You would have made a LOSS of around &lt;span style=&quot;color:red&quot;&gt;INR 3100&lt;/span&gt;!!

-------------

## Conclusion

This is how exactly an SIP works!

You buy right and sit tight. Always accumulate slowly (not daily :wink: )!

--------------</content><author><name>Sukeesh</name></author><summary type="html"></summary></entry><entry><title type="html">Setting up and Troubleshooting Prometheus in Kubernetes</title><link href="http://localhost:4000/blog/2020-03-05/prometheus" rel="alternate" type="text/html" title="Setting up and Troubleshooting Prometheus in Kubernetes" /><published>2020-03-05T00:00:00+05:30</published><updated>2020-03-05T00:00:00+05:30</updated><id>http://localhost:4000/blog/2020-03-05/prometheus</id><content type="html" xml:base="http://localhost:4000/blog/2020-03-05/prometheus">----------
### Prometheus

Prometheus is an open-source systems monitoring and alerting toolkit. It scrapes metrics from instrumented jobs, either directly or via an intermediary push gateway for short-lived jobs. It stores all scraped samples locally and runs rules over this data to either aggregate and record new time series from existing data or generate alerts. [link](https://prometheus.io/docs/introduction/overview/)


I tried to make this blog post as simple as possible and it has all the basic components required to setup Prometheus successfully in your microservice.

----

### Architecture

Ignore this section if it's too confusing to understand

![Prom](https://prometheus.io/assets/architecture.png &quot;Architecture&quot;) [ref](https://prometheus.io/docs/introduction/overview/)

----

### Prometheus Client (Python)

```py
from prometheus_client import start_http_server, Counter
import random
import time

c = Counter('count', 'Description of counter')

if __name__ == '__main__':
    start_http_server(8000)
    while True:
      c.inc(0.1)
      time.sleep(random.random())
```

Above is a simple http server which has a Prometheus Counter implemented. Counter is incremented by `0.1` everytime `c.inc(0.1)` is called. Run the above program and try to curl `localhost:8000/metrics` and you should see the following metrics being served.


```
$ curl localhost:8000/metrics

# HELP python_info Python platform information
# TYPE python_info gauge
python_info{implementation=&quot;CPython&quot;,major=&quot;3&quot;,minor=&quot;7&quot;,patchlevel=&quot;6&quot;,version=&quot;3.7.6&quot;} 1.0
# HELP python_gc_collected_objects Objects collected during gc
# TYPE python_gc_collected_objects histogram
# HELP python_gc_uncollectable_objects Uncollectable object found during GC
# TYPE python_gc_uncollectable_objects histogram
# HELP python_gc_duration_seconds Time spent in garbage collection
# TYPE python_gc_duration_seconds histogram
# HELP count_total Some description
# TYPE count_total counter
count_total 0.7
# TYPE count_created gauge
count_created 1583416778.013898
```

---------------
### Tip

To check if above metrics are in valid format, you can use `promtool` to validate

```yaml
$ curl localhost:8000/metrics | promtool check metrics
```
-----------------------

### Storage

Prometheus includes a local on-disk time series database, but also optionally integrates with remote storage systems.

Here, let's focus on local storage and we need to provide some dir path to Prometheus to write metrics to and for that to write, we need to set environment variable `prometheus_multiproc_dir` (default) to some temporary dir like `/tmp`.


Some examples of remote storage include [Prometheus Pushgateway](https://github.com/prometheus/pushgateway). Used to allow ephemeral and batch jobs to expose their metrics to Prometheus. Since these kinds of jobs may not exist long enough to be scraped, they can instead push their metrics to a Pushgateway. The Pushgateway then exposes these metrics to Prometheus.

-------------------

### Prometheus scrape

Now, that these metrics are being served, we somehow need to tell Prometheus server to scrape these metrics from these pods.

For the above purpose, we need to have per pod Prometheus annotations which allow a fine control of the scraping process. These annotations need to be part of pod metadata. 

Annotations required:

- `prometheus.io/scrape` Default config will scrape all pods (true/false)
- `prometheus.io/path` If the metrics path is not /metrics, define it with this annotation.
- `prometheus.io/port`

-------
### Example

In our case, K8s config will look like below

```yaml
...
spec:
  template:
    metadata:
      annotations:
        prometheus.io/scrape: 'true'
        prometheus.io/port: '8000'
  containers:
    env:
    - name: prometheus_multiproc_dir
      value: /tmp
...
```
-----

### Conclusion

Above should export all the metrics on pods to prometheus server. You can check on Prometheus targets and verify if these are being exported.

Now, this data can be consumed to show visualizations like on Grafana. :tada:


If still stuck while setting up, Hire me on [Codementor.io](http://codementor.io/sukeesh) :wink:</content><author><name>Sukeesh</name></author><summary type="html">Prometheus</summary></entry><entry><title type="html">Started Contributing to Kubernetes</title><link href="http://localhost:4000/blog/2019-08-01/started-kubernetes" rel="alternate" type="text/html" title="Started Contributing to Kubernetes" /><published>2019-08-01T00:00:00+05:30</published><updated>2019-08-01T00:00:00+05:30</updated><id>http://localhost:4000/blog/2019-08-01/started-kubernetes</id><content type="html" xml:base="http://localhost:4000/blog/2019-08-01/started-kubernetes">So, finally started [contributing to the Kubernetes source code on github](https://github.com/kubernetes/kubernetes/pulls/sukeesh). At first, I was really worried after looking at the source code as It was too huge. But finally, I made up my mind to start with [good first issues](https://github.com/kubernetes/kubernetes/issues?q=is%3Aopen+is%3Aissue+label%3A%22good+first+issue%22). 

Most of the good first issues were to cleanup the source code, add docs in the code kind of stuff and seemed really easy to me. So, came up with this PR [#79367](https://github.com/kubernetes/kubernetes/pull/79367) to start off. The code review flow was really good (even though took a bit time) and this PR was finally merged by the bot. :wink:

![Merged](../assets/images/july2019/merge.png &quot;Yaay&quot;)

I was really happy on that day. I was little more encouraged to contribute more. So, now came up with this huge PR[#79566](https://github.com/kubernetes/kubernetes/pull/79566), which does solve the same issue of capitalizing error strings. I changed the code in various places (31 files). So, it involved various packages:cry: and this really took a long time to get the code approved :ballot_box_with_check: as there are different maintainers for each package.

![Labels](../assets/images/july2019/labels_huge.png &quot;Too many&quot;)


So, now that I'm little familiar with this, I was excited to fix some more real bugs/issues. I actually found a small bug wherein Horizontal Pod Autoscaler was showing incorrect status in the events log. 
For example, the case when
```go
desiredReplicas = 1
minimumAllowedReplicas = 2
hpaMinReplicas = 2
```
Event logs said 
```
TooFewReplicas
the desired replica count is more than the maximum replica count
```
which defnitely is wrong and when I read the code realted to autoscaler part, I found the issue:sweat_smile:&lt;br&gt;
I, Immediately came up with this PR [#79859](https://github.com/kubernetes/kubernetes/pull/79859). &lt;br&gt; 
I was very happy at the point when this PR was merged. So, now again started to look over some of the open issues on github and no luck this time. But found some issue where this guy very new to Kubernetes was having hard time [Issue #79678](https://github.com/kubernetes/kubernetes/issues/79678)

So, was thinking of my next contribution and came up with the feature of flushing out the current namespace when there are no resources found [#79968](https://github.com/kubernetes/kubernetes/pull/79968) :smile:

I made these contributions in under a month and looking forward to contributing more to CNCF Projects.

I have also made a habit of reading the source code, If I have a doubt regarding anything related to kubernetes and I feel like this is the best way to learn.</content><author><name>Sukeesh</name></author><summary type="html">So, finally started contributing to the Kubernetes source code on github. At first, I was really worried after looking at the source code as It was too huge. But finally, I made up my mind to start with good first issues.</summary></entry><entry><title type="html">Installing helm chart</title><link href="http://localhost:4000/blog/2019-07-02/helm-init" rel="alternate" type="text/html" title="Installing helm chart" /><published>2019-07-02T00:00:00+05:30</published><updated>2019-07-02T00:00:00+05:30</updated><id>http://localhost:4000/blog/2019-07-02/helm-init</id><content type="html" xml:base="http://localhost:4000/blog/2019-07-02/helm-init">`$ helm init`

`$ helm create test-sukeesh`

This will create all the files necessary, We are only interested in `Chart.yaml`, `values.yaml` files.

`$ helm lint` this would lint all the files

Once you are done with all the changes, then go back to the root dir and do `helm install`

`$ cd ..`
`$ helm install test-sukeesh-0.1.0.tgz`

But, when you try above command, you might see some error related to invalid permissions.

So, when you try to tail the logs of tiller pod in `kube-system` namespace which is installed when we did `helm init`
We see these errors like these
```
tiller-deploy-c78f4c8d9-2q4jz tiller [tiller] 2019/07/02 07:56:00 preparing install for
tiller-deploy-c78f4c8d9-2q4jz tiller [storage] 2019/07/02 07:56:00 getting release &quot;orbiting-hog.v1&quot;
tiller-deploy-c78f4c8d9-2q4jz tiller [storage/driver] 2019/07/02 07:56:00 get: failed to get &quot;orbiting-hog.v1&quot;: configmaps &quot;orbiting-hog.v1&quot; is forbidden: User &quot;system:serviceaccount:kube-system:default&quot; cannot get resource &quot;configmaps&quot; in API group &quot;&quot; in the namespace &quot;kube-system&quot;
```
So, by default when we don't specify a service account in the service, this uses `&quot;system:serviceaccount:kube-system:default&quot;` service account but then this has no permissions.
So, create a Role-based access control file
`rbac-config.yaml`

```yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: tiller
  namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: tiller
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
  - kind: ServiceAccount
    name: tiller
    namespace: kube-system
```
Apply this `rbac-config.yaml` file

```
$ kubectl create -f rbac-config.yaml
serviceaccount &quot;tiller&quot; created
clusterrolebinding &quot;tiller&quot; created
```
You can see serviceaccount and clusterrolebinding &quot;tiller&quot; has been created successfully.
Verify with the following
```
$ kubectl get serviceaccounts | grep tiller
tiller                               1         30s
```
But, when you try installing the helm, you might again get the same error because we created a service account but did not associate it with our tiller deployment.

```
$ kubectl get deploy | grep tiller
tiller-deploy            1         1         1            1           91m
```

```
$ kubectl edit deploy tiller-deploy
deployment.extensions/tiller-deploy edited
```
Edit the `tiller-deploy` deployment and add serviceAccountName to spec.template.spec as below
```
...
serviceAccountName: tiller
...
```

Now, helm is able to install with no error

```
$ helm install test-sukeesh-0.1.0.tgz --debug

[debug] Created tunnel using local port: '52030'

[debug] SERVER: &quot;127.0.0.1:52030&quot;

[debug] Original chart version: &quot;&quot;
[debug] CHART PATH: /Users/sukeesh/Documents/helm/test-sukeesh-0.1.0.tgz

NAME:   alliterating-wildebeest
REVISION: 1
RELEASED: Tue Jul  2 21:27:15 2019
CHART: test-sukeesh-0.1.0
USER-SUPPLIED VALUES:
{}

COMPUTED VALUES:
affinity: {}
image:
  pullPolicy: IfNotPresent
  repository: daemonza/testapi
  tag: latest
ingress:
  annotations: {}
  enabled: false
  hosts:
  - chart-example.local
  path: /
  tls: []
nodeSelector: {}
replicaCount: 2
resources:
  limits:
    cpu: 100m
    memory: 128Mi
  requests:
    cpu: 100m
    memory: 128Mi
service:
  externalPort: 80
  internalPort: 80
  name: test-sukeesh
  port: 9000
  type: ClusterIP
tolerations: []

HOOKS:
MANIFEST:

---
# Source: test-sukeesh/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: alliterating-wildebeest-test-sukeesh
  labels:
    app: test-sukeesh
    chart: test-sukeesh-0.1.0
    release: alliterating-wildebeest
    heritage: Tiller
spec:
  type: ClusterIP
  ports:
    - port: 9000
      targetPort: http
      protocol: TCP
      name: http
  selector:
    app: test-sukeesh
    release: alliterating-wildebeest
---
# Source: test-sukeesh/templates/deployment.yaml
apiVersion: apps/v1beta2
kind: Deployment
metadata:
  name: alliterating-wildebeest-test-sukeesh
  labels:
    app: test-sukeesh
    chart: test-sukeesh-0.1.0
    release: alliterating-wildebeest
    heritage: Tiller
spec:
  replicas: 2
  selector:
    matchLabels:
      app: test-sukeesh
      release: alliterating-wildebeest
  template:
    metadata:
      labels:
        app: test-sukeesh
        release: alliterating-wildebeest
    spec:
      containers:
        - name: test-sukeesh
          image: &quot;daemonza/testapi:latest&quot;
          imagePullPolicy: IfNotPresent
          ports:
            - name: http
              containerPort: 80
              protocol: TCP
          livenessProbe:
            httpGet:
              path: /
              port: http
          readinessProbe:
            httpGet:
              path: /
              port: http
          resources:
            limits:
              cpu: 100m
              memory: 128Mi
            requests:
              cpu: 100m
              memory: 128Mi
LAST DEPLOYED: Tue Jul  2 21:27:15 2019
NAMESPACE: kube-system
STATUS: DEPLOYED

RESOURCES:
==&gt; v1/Service
NAME                                  TYPE       CLUSTER-IP     EXTERNAL-IP  PORT(S)   AGE
alliterating-wildebeest-test-sukeesh  ClusterIP  10.59.245.130  &lt;none&gt;       9000/TCP  1s

==&gt; v1beta2/Deployment
NAME                                  DESIRED  CURRENT  UP-TO-DATE  AVAILABLE  AGE
alliterating-wildebeest-test-sukeesh  2        2        2           0          0s

==&gt; v1/Pod(related)
NAME                                                   READY  STATUS             RESTARTS  AGE
alliterating-wildebeest-test-sukeesh-6d577d4888-bftnq  0/1    ContainerCreating  0         0s
alliterating-wildebeest-test-sukeesh-6d577d4888-ntmzz  0/1    ContainerCreating  0         0s


NOTES:
1. Get the application URL by running these commands:
  export POD_NAME=$(kubectl get pods --namespace kube-system -l &quot;app=test-sukeesh,release=alliterating-wildebeest&quot; -o jsonpath=&quot;{.items[0].metadata.name}&quot;)
  echo &quot;Visit http://127.0.0.1:8080 to use your application&quot;
  kubectl port-forward $POD_NAME 8080:80
```</content><author><name>Sukeesh</name></author><summary type="html">$ helm init</summary></entry><entry><title type="html">Dropbox Deduplication test</title><link href="http://localhost:4000/blog/2017-09-01/dropbox-deduplication" rel="alternate" type="text/html" title="Dropbox Deduplication test" /><published>2017-09-01T00:00:00+05:30</published><updated>2017-09-01T00:00:00+05:30</updated><id>http://localhost:4000/blog/2017-09-01/dropbox-deduplication</id><content type="html" xml:base="http://localhost:4000/blog/2017-09-01/dropbox-deduplication">It works by breaking files into blocks. Each of these blocks is hashed. Only blocks that are not yet known are uploaded to the server when syncing.

Check : [Dropbox blog](https://blogs.dropbox.com/dropbox/2011/07/changes-to-our-policies/)
### Test

I used this [video](https://www.youtube.com/watch?v=FBCmIFCpJBU) for testing which is `21.8 MB` in size.
The freshly downloaded file took 27 seconds ( approx ) to sync. Then, I renamed the file. This duplicate file took 4 seconds( approx )!

### Video

[![some random](https://img.youtube.com/vi/dDvfkiUxS-A/0.jpg)](https://www.youtube.com/watch?v=dDvfkiUxS-A)</content><author><name>Sukeesh</name></author><summary type="html">It works by breaking files into blocks. Each of these blocks is hashed. Only blocks that are not yet known are uploaded to the server when syncing.</summary></entry><entry><title type="html">Lowest Common Ancestor | Binary Lifting</title><link href="http://localhost:4000/blog/2016-08-22/lca-binary-lifting" rel="alternate" type="text/html" title="Lowest Common Ancestor | Binary Lifting" /><published>2016-08-22T00:00:00+05:30</published><updated>2016-08-22T00:00:00+05:30</updated><id>http://localhost:4000/blog/2016-08-22/lca-binary-lifting</id><content type="html" xml:base="http://localhost:4000/blog/2016-08-22/lca-binary-lifting">&lt;b&gt;What is LCA?&lt;/b&gt;

In graph theory and computer science, the lowest common ancestor (LCA) of two nodes v and w in a tree or directed acyclic graph (DAG) T is the lowest (i.e. deepest) node that has both v and w as descendants, where we define each node to be a descendant of itself (so if v has a direct connection from w, w is the lowest common ancestor).

LCA can be found in _logN_ time with _N * logN_ preprocessing.

---------

Variable | Description
------------- |---------
_L[i]_    | Level of node _i_ 
_P[i]_    | Parent of node _i_
_LCA[i][j]_ | 2&lt;sup&gt;j -th&lt;/sup&gt; ancestor of node _i_


----------

## Preprocessing

&lt;b&gt;Finding level and parent of each node using simple dfs&lt;/b&gt;

```cpp
void dfs(int node, int par){
    L[node] = L[par] + 1;
    par[node] = par;
    for ( int j = 0 ; j &lt; adj[node].size() ; j ++ ){
        int to = adj[node][j];
        if(to != par) dfs(to, node);
    }
}
```

&lt;b&gt;Filling in LCA array&lt;/b&gt;

&gt; Note: Any non negative number can be uniquely represented as a sum of decreasing powers of 2. This is just a variant of binary representation of a number. For number x, there can be at most log2(x) summands

We need to compute `LCA` table. For computing this value we may use following recursion

```cpp
LCA[i][j] = LCA[LCA[i][j-1]][j-1],  if j &gt; 0
----
LCA[i][j] = P[i],  if j = 0
```

if j &gt; 0,&lt;br&gt; _LCA[i][j]_ means 2&lt;sup&gt;j -th&lt;/sup&gt; ancestor of node _i_, which is nothing but 2&lt;sup&gt;(j-1) -th&lt;/sup&gt; ancestor of node _LCA[i][j-1]_

By using above recursion, we build LCA array as following
```cpp
void ConstructLCA(int n){
    lg = ceil(log2(n));
    int i, j;
    for ( i = 1 ; i &lt;= n ; i ++ ) LCA[i][0] = par[i];

    for ( i = 1 ; i &lt;= lg ; i ++ ) {
        for( j = 1 ; j &lt;= n ; j ++ ) {
            if( LCA[j][i-1] ) {
                LCA[j][i] = LCA[ LCA[j][i-1] ] [i-1];
            }
        }
    }
}
```

## Query

So, for every power j of 2, if `LCA[x][j] != LCA[y][j]` then we know that LCA(x, y) is on a higher level and we will continue searching for `LCA(x = LCA[x][j], y = LCA[y][j])`. At the end, both x and y will have the same father, so return P[x]. Let’s see what happens if `L[x] != L[y]`. Assume, without loss of generality, that L[p] &lt; L[q]. We can use the same meta-binary search for finding the ancestor of p situated on the same level with q, and then we can compute the LCA as described below.

```cpp
int getLca(int x, int y){
    
    if(L[x] &lt; L[y]){
        swap(x, y);
    }

    for(int i=lg; i &gt;= 0; i--){
        if( LCA[x][i] != 0 &amp;&amp; L[LCA[x][i]] &gt;= L[y] ){
            x = LCA[x][i];
        }
    }
    
    if( x == y ){
        return x;
    }

    for(int i = lg ; i &gt;= 0 ; i--){
        if( LCA[x][i] != 0 &amp;&amp; LCA[x][i] != LCA[y][i] ){
            x = LCA[x][i];
            y = LCA[y][i];
        }
    }

    return LCA[x][0];
}
```

We can observe that this function makes at most `2 * log(H)` operations (H is the height of the tree).



&lt;b&gt;My Implementation in C++&lt;/b&gt;&lt;br&gt;
[github/sukeesh/Algorithm-Implementations/graphs/LCA/LCAbinarylifting.cpp](https://github.com/sukeesh/Algorithm-Implementations/blame/master/graphs/LCA/LCAbinarylifting.cpp)

&lt;b&gt;Reference&lt;/b&gt;
[Topcoder tutorial](https://www.topcoder.com/community/competitive-programming/tutorials/range-minimum-query-and-lowest-common-ancestor/)</content><author><name>Sukeesh</name></author><summary type="html">What is LCA?</summary></entry><entry><title type="html">Sparse tables</title><link href="http://localhost:4000/blog/2016-01-14/sparse-table" rel="alternate" type="text/html" title="Sparse tables" /><published>2016-01-14T00:00:00+05:30</published><updated>2016-01-14T00:00:00+05:30</updated><id>http://localhost:4000/blog/2016-01-14/sparse-table</id><content type="html" xml:base="http://localhost:4000/blog/2016-01-14/sparse-table">What are sparse tables? 

- Data structure that allows range queries
- Can answer most queries in O(log N)
- But can answer Range Minimum Queries in O(1) 
    - An idempotent operation can be repeated an arbitrary number of times and the result will be the same as if it had been    done only once. In arithmetic, adding zero to a number is idempotent.
- It can be only used on immutable arrays. If you need any change, whole DS needs to be changed

Intuition

- Any non negative number can be uniquely represented as a sum of decreasing powers of 2. This is just a variant of binary representation of a number. For number x, there can be at most log2(x) summands
- So, by the same reasoning, any interval can be represented as a union of intervals with lengths that are decreasing powers of two. Eq.. [2, 14] = [2, 9] U [10, 13] U [14, 14]
- And hence, also here union consists of at most [log2(length of interval)] many intervals.
- MAIN IDEA: Pre-compute all answers for range queries with power of 2 length. So, afterwards combining all of them to receive a complete answer.


Precomputation

- We will maintain a 2D array
- Where table[i][j] will store the answer for the range [i, i + 2^j - 1] which can be split nicely into ranges 
    - [i, i + 2^{j-1} - 1] and [i + 2^(j-1), i + 2^j - 1] both of length 2^{j-1}
    - So, we can generate this table with dynamic programming

`K = log2(MAXN) + 1`

logarithmic values

```cpp
int log[MAXN+1];
log[1] = 0;
for (int i = 2; i &lt;= MAXN; i++)
    log[i] = log[i/2] + 1;
```

computing sparse table

```cpp
for (int i = 0; i &lt; MAXN; i++)
    table[i][0] = f(array[i]);

for (int j = 1; j &lt;= K; j++)
    for (int i = 0; i + (1 &lt;&lt; j) &lt;= MAXN; i++)
        table[i][j] = f(table[i][j-1], table[i + (1 &lt;&lt; (j - 1))][j - 1]);
```

Query

- for sum (between L and R)

```cpp
int sum = 0;
for (int j = K; j &gt;= 0; j--) {
    if ((1 &lt;&lt; j) &lt;= R - L + 1) {
        sum += table[L][j];
        L += 1 &lt;&lt; j;
    }
}
```

- for RMQ (Minimum between L and R)

```cpp
int j = log[R - L + 1];
int minimum = min(table[L][j], table[R - (1 &lt;&lt; j) + 1][j]);
```</content><author><name>Sukeesh</name></author><summary type="html">What are sparse tables?</summary></entry><entry><title type="html">Simple script to hack during contests</title><link href="http://localhost:4000/blog/2015-12-18/codeforces-hack" rel="alternate" type="text/html" title="Simple script to hack during contests" /><published>2015-12-18T00:00:00+05:30</published><updated>2015-12-18T00:00:00+05:30</updated><id>http://localhost:4000/blog/2015-12-18/codeforces-hack</id><content type="html" xml:base="http://localhost:4000/blog/2015-12-18/codeforces-hack">This is a simple shell script to hack during codeforces/topcoder rounds. `your_code.cpp` should be your code, `victim_code.cpp` should be victim's code. Write test case generator in `test.py` and run this bash file by `bash hack.sh`. This script keeps on iterating until it finds a test case where your_code.cpp and victim_code.cpp gives a different output. Once it finds a hack test case, this stops iterating. Now, you can copy the test case and hack.

### Happy Hacking!


```bash
while true
do
python test.py &gt; in
g++ your_code.cpp
./a.out &lt;in&gt; sukeesh
g++ victim_code.cpp
./a.out &lt;in&gt; victim
diff --brief &lt;(sort victim) &lt;(sort sukeesh) &gt;/dev/null
comp_value=$?
if [ $comp_value -eq 1 ]
then
	echo &quot; &quot;
	echo &quot;--------- HACK! --------&quot;
	echo &quot; Input : &quot;
	cat ./in
	echo &quot; Expected : &quot;
	cat ./sukeesh
	printf &quot;\n&quot;
	echo &quot; Recieved : &quot;
	cat ./victim
	printf &quot;\n&quot;
	break
else
	echo &quot; &quot;
	echo &quot;----------Same----------&quot;
	echo &quot; Input : &quot;
	cat ./in
	echo &quot; Expected : &quot;
	cat ./sukeesh
	printf &quot;\n&quot;
	echo &quot; Recieved : &quot;
	cat ./victim
	printf &quot;\n&quot;
fi
done

```

This is the script</content><author><name>Sukeesh</name><email>vsukeeshbabu@gmail.com</email></author><summary type="html">This is a simple shell script to hack during codeforces/topcoder rounds. your_code.cpp should be your code, victim_code.cpp should be victim’s code. Write test case generator in test.py and run this bash file by bash hack.sh. This script keeps on iterating until it finds a test case where your_code.cpp and victim_code.cpp gives a different output. Once it finds a hack test case, this stops iterating. Now, you can copy the test case and hack.</summary></entry></feed>